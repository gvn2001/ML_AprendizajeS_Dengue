# ğŸ“Š PredicciÃ³n de Casos de Dengue mediante Aprendizaje Supervisado  

## ğŸ“Œ DescripciÃ³n  
Este proyecto implementa algoritmos de **aprendizaje supervisado** para predecir la cantidad de casos de dengue en dos ciudades, basado en datos de la competiciÃ³n [DrivenData](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/).  

### ğŸ” MetodologÃ­a  
1. **ExploraciÃ³n y selecciÃ³n de caracterÃ­sticas**  
2. **Entrenamiento de distintos modelos**  
3. **OptimizaciÃ³n de hiperparÃ¡metros (Random Search y Grid Search)**  
4. **EvaluaciÃ³n mediante el Error Absoluto Medio (MAE)**  

### ğŸ” Principales hallazgos  
- **Modelos simples superaron a los mÃ¡s complejos** debido a problemas en la optimizaciÃ³n de hiperparÃ¡metros.  
- **Ãrboles de DecisiÃ³n (AD) fue el mejor modelo**, obteniendo el menor MAE.  

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas  

### ğŸ“Œ Lenguaje de ProgramaciÃ³n  
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

### ğŸ“Œ LibrerÃ­as Principales  
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white) ![Scikit-Learn](https://img.shields.io/badge/Scikit%20Learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?style=for-the-badge&logo=plotly&logoColor=white) ![Seaborn](https://img.shields.io/badge/Seaborn-008080?style=for-the-badge&logo=python&logoColor=white)


---

## ğŸ“‚ Estructura del Proyecto  

ğŸ“„ models.ipynb â†’ Notebook que contiene el desarrollo completo del proyecto  

---

## ğŸš€ Modelos Evaluados  
Se evaluaron distintos modelos con los siguientes resultados:  

| Modelo   | MAE test (DrivenData) | MAE validaciÃ³n | Algoritmo | HiperparÃ¡metros |
|----------|-----------------------|---------------|-----------|----------------|
| **Ejemplo-RL** | 27,2524 | NO | RegresiÃ³n Lineal | NO |
| **KNN** | 27,550 | 21.9103 | K-Nearest Neighbors | metric = euclidean; n_neighbors = 62; weights = distance |
| **AD** âœ… | **26,7716** | **20.9725** | Ãrboles de DecisiÃ³n | max_depth = 3; max_features = sqrt; min_samples_leaf = 13; min_samples_split = 8 |
| **CM-RF** | 28,9663 | 22.3295 | Random Forest | max_depth = 18; max_features = log2; min_samples_leaf = 18; min_samples_split = 2; n_estimators = 256 |
| **GB** | 28,0240 | 21.2116 | Gradient Boosting | n_estimators = 30 |
| **RR** | 27,2620 | 22.6635 | RegresiÃ³n Ridge | alpha = 2 |

---

## ğŸ† Modelo Final Elegido  
El modelo seleccionado para la predicciÃ³n final fue **Ãrboles de DecisiÃ³n (AD)** debido a su mejor rendimiento.  

ğŸ“Œ **Detalles del Modelo Final:**  
- **MAE test (DrivenData):** 26,7716  
- **MAE validaciÃ³n:** 20.9725  
- **HiperparÃ¡metros:** max_depth = 3; max_features = sqrt; min_samples_leaf = 13; min_samples_split = 8  
- **CaracterÃ­sticas usadas:** station_diur_temp_rng_c, station_avg_temp_c, precipitation_amt_mm, station_precip_mm, reanalysis_relative_humidity_percent, station_min_temp_c  
- **OptimizaciÃ³n:** Random Search y Grid Search con validaciÃ³n cruzada  

---

## ğŸ“ˆ Conclusiones y Aspectos de Mejora  

âœ” **Los modelos mÃ¡s simples obtuvieron mejores resultados**: Se esperaba que algoritmos mÃ¡s complejos como **Random Forest (CM-RF)** o **Gradient Boosting (GB)** fueran superiores, pero la optimizaciÃ³n de hiperparÃ¡metros resultÃ³ desafiante.  
âœ” **La selecciÃ³n de caracterÃ­sticas podrÃ­a mejorarse**: Probar tÃ©cnicas mÃ¡s avanzadas podrÃ­a mejorar el rendimiento.  
âœ” **El dataset es limitado**: Con mÃ¡s datos, los modelos podrÃ­an generalizar mejor y mejorar sus predicciones.  



